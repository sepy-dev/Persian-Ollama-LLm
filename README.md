# Persian-Ollama-LLm
Razor-AI โ ฺุช ูุญู ุจุง Ollama (ูุงุฑุณ โ ุงูฺฏูุณุ ุขููุงู/ุจููู)












โก ุฎูุงุตู (ูุงุฑุณ โ ุงูู)

Razor-AI ฺฉ ูุงูู ูุงุณุท ุงุณุช ุจุฑุง ุจูุจูุฏ ฺฉูุช ุชุนุงูู ุจุง ูุฏูโูุง Ollama ุจุฑุง ฺฉุงุฑุจุฑุงู ูุงุฑุณโุฒุจุงู. ูุดฺฉู: ุจุณุงุฑ ุงุฒ ูุฏูโูุง (ูุฎุตูุตุงู ูุฏูโูุง ฺฉุฏููุณ) ูุณุจุช ุจู ูุฑูุฏโูุง ุงูฺฏูุณ ุจูุชุฑ ุนูู ูโฺฉููุฏ. ุฑุงูฺฉุงุฑ: ูุฑูุฏ ูุงุฑุณ (ุงุฎุชุงุฑ) ุจูโุตูุฑุช ููฺฉุงู ุจุง ูุฏู quickmt-fa-en (CTranslate2) ุชุฑุฌููู ูุงุฑุณ โ ุงูฺฏูุณ ูโุดูุฏุ ุณูพุณ ูุชู ุงูฺฏูุณู ุชุฑุฌููโุดุฏู ุจู Ollama ูุฑุณุชุงุฏู ูโุดูุฏ. ุจููฺฉโูุง ฺฉุฏ (``` ... ```) ู inline code ุฏุณุชโูุฎูุฑุฏู ูโูุงููุฏ ุชุง ุณุงุฎุชุงุฑ ฺฉุฏ ุญูุธ ุดูุฏ.

ุงู README ุดุงูู ุฏุณุชูุฑุงูุนููโูุง ฺฉุงูู ุจุฑุง:

ุฑุงูโุงูุฏุงุฒ ุจุง Docker Composeุ

ุฏุงูููุฏ ุฎูุฏฺฉุงุฑ ุง ุฏุณุช ูุฏู ุชุฑุฌููุ

ูุนุงูโุณุงุฒ GPU ุจุฑุง Ollamaุ

ุฑุงูููุง ุชุบุฑ ูุฏู Ollama ู ูฺฉุงุช ุนูู.

๐ ูุฏู ูพุฑูฺู

ุจุงูุง ุจุฑุฏู ฺฉูุช ูพุงุณุฎโุฏูู ูุฏูโูุง Ollama ุจุฑุง ูุฑูุฏโูุง ูุงุฑุณ

ุญูุธ ุณุงุฎุชุงุฑ ู ฺฉุฏ ุฏุงุฎู prompt (ูุงููุฏ fenced code block)

ุงุฌุฑุง ุดุฏู ุชุฑุฌูู ุจูโุตูุฑุช ุขููุงู ู ููฺฉุงู ุจุง CTranslate2 (ุจุฏูู ูุงุฒ ุจู ุงูุชุฑูุช ุจุนุฏ ุงุฒ ุฏุงูููุฏ ูุฏู)

ุฑุงุจุท ฺฉุงุฑุจุฑ ุณุงุฏู (FastAPI + SPA) ุจุง ุงูฺฉุงู ููุงุด ุชุฑุฌูู ู ูพุงู ูุฏู

โ ูุงุจูุชโูุง

ุชุฑุฌููู ูุญู Fa โ En ุจุง quickmt-fa-en (CTranslate2)

ุญูุธ ุจููฺฉโูุง ฺฉุฏ ู inline code

UI ุฒุจุง ู ูุงุจูุช ฺฉูพ ุจููฺฉู ฺฉุฏ

ุฏุงูููุฏ ุฎูุฏฺฉุงุฑ ูุฏู ุชุฑุฌูู ุฏุฑ entrypoint (ุฏุฑ ุตูุฑุช ูุจูุฏ ูุฏู)

ูพุดุชุจุงู GPU ุจุฑุง Ollama (ุฏุฑ ุตูุฑุช ููุฌูุฏ ุจูุฏู ุณุฎุชโุงูุฒุงุฑ ู ุชูุธูุงุช ูุฒุจุงู)

ุงูฺฉุงู ุชุบุฑ ูุฏู Ollama ุจุง ูุฑุงุด ฺฉ ูุชุบุฑ ูุญุท (OLLAMA_MODEL)

ุฑุงูููุง ุณุฑุน โ ุงุฌุฑุง ุจุง Docker

ุณุงุฎุช ุงูุฌ ู ุจุงูุง ุขูุฑุฏู ุณุฑูุณโูุง:

docker compose build --no-cache
docker compose up -d


ูุฑูุฑฺฏุฑ ุฑุง ุจุงุฒ ฺฉู: http://localhost:8000

ุจุฑุง ุชุบุฑ ูุฏู Ollamaุ ูุชุบุฑ ูุญุท OLLAMA_MODEL ุฑุง ูุฑุงุด ฺฉู (ุฏุฑ docker-compose.yml ุง ูุจู ุงุฒ ุงุฌุฑุง ฺฉุงูุชูุฑ):

environment:
  - OLLAMA_MODEL=qwen2.5-coder:1.5b


ุงฺฏุฑ ูโุฎูุงู Ollama ุฑุง ุจุง GPU ุงุฌุฑุง ฺฉูุ ุจุจู ุจุฎุด GPU ูพุงูโุชุฑ.

ูุงูโูุง ููููู (ูุงุจู ฺฉูพ)
ููููู docker-compose.yml

ุฏุฑ ุงู ููููู Ollama ู razor-api ุฏุฑ ุฏู ุณุฑูุณ ุฌุฏุง ุงุฌุฑุง ูโุดููุฏ. ุงฺฏุฑ ูโุฎูุงู GPU ุฑุง ูุนุงู ฺฉูุ ุฑุงูููุง GPU ุฑุง ุจุจู.

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_USE_GPU=1   # ุฏุฑ ุตูุฑุช ูุงุฒ (ุฑู ูุฒุจุงู ูุตุจ GPU ูุงุฒู ุงุณุช)
    # ูฺฉุชู: ุจุฑุง ุงุฌุฑุง ุจุง GPU ูโุชูุงู ุงุฒ `--gpus all` ุง ุชูุธูุงุช nvidia runtime ุงุณุชูุงุฏู ฺฉุฑุฏ.

  razor-api:
    build: .
    container_name: razor-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=qwen2.5-coder:1.5b
    volumes:
      - ./app:/app   # ุจุฑุง ุชูุณุนู: ุชุบุฑุงุช ููฺฉุงู ุฑุง ููุนฺฉุณ ูโฺฉูุฏ

volumes:
  ollama_data:

ููููู Dockerfile ุจุฑุง razor-api

(ุงู ููููู ุฏุงูููุฏ ูุณุชูู ูุฏู ุฏุงุฎู image ุฑุง ุงูุฌุงู ููโุฏูุฏ โ ุฏุงูููุฏ ุฏุฑ runtime ุงูุฌุงู ูโุดูุฏ ุชุง ุญุฌู image ุฒุงุฏ ูุดูุฏ.)

FROM python:3.11-slim

WORKDIR /app
COPY ./app /app
# ุงฺฏุฑ quickmt repo ูุญู ุฑุง ุฏุงุฑุ ุขู ุฑุง ูู ฺฉูพ ฺฉู:
COPY ./quickmt /app/quickmt

# ูุตุจ ูุงุจุณุชฺฏโูุง
RUN apt-get update && apt-get install -y git curl --no-install-recommends \
  && pip install --no-cache-dir fastapi uvicorn httpx jinja2 python-multipart requests ctranslate2 sentencepiece ollama \
  && pip install --no-cache-dir /app/quickmt || true

# ูุทูุฆู ุดู entrypoint ุงุฌุฑุงู
RUN chmod +x /app/entrypoint.sh

EXPOSE 8000
CMD ["/app/entrypoint.sh"]

ููููู entrypoint.sh

(ุงู ูุงู ุฏุฑ ุฒูุงู runtime ุงุฌุฑุง ูโุดูุฏุ ุงฺฏุฑ ูุฏู ุชุฑุฌูู ููุฌูุฏ ูุจุงุดุฏ ุขู ุฑุง ุฏุงูููุฏ ูโฺฉูุฏ ุณูพุณ uvicorn ุฑุง ุงุฌุฑุง ูโฺฉูุฏ.)

#!/usr/bin/env sh
set -eu

MODEL_DIR="/app/quickmt-fa-en"

# ุงฺฏุฑ quickmt ูุญู ุฏุงุฎู ุงูพ ูุณุชุ ูุตุจุด ฺฉู (ุงุฎุชุงุฑ)
if [ -d "/app/quickmt" ]; then
  pip install --no-cache-dir /app/quickmt || true
fi

# ุฏุงูููุฏ ูุฏู ุชุฑุฌูู ุฏุฑ ุตูุฑุช ุนุฏู ูุฌูุฏ ุง ุฎุงู ุจูุฏู ูพูุดู
if [ ! -d "${MODEL_DIR}" ] || [ -z "$(ls -A ${MODEL_DIR} 2>/dev/null)" ]; then
  echo "โก Downloading quickmt-fa-en (this can take several minutes)..."
  # ุงฺฏุฑ quickmt-model-download ูุตุจ ุดุฏู ุจุงุดุฏ:
  quickmt-model-download quickmt/quickmt-fa-en ${MODEL_DIR} || echo "quickmt-model-download failed; continuing"
else
  echo "Model exists, skipping download."
fi

# ุฏุฑ ููุงุช ุจุฑูุงูู ุฑุง ุงุฌุฑุง ฺฉู
exec uvicorn main:app --host 0.0.0.0 --port 8000


ูฺฉุชู: ุงฺฏุฑ ูโุฎูุงู ุฏุงูููุฏ ูุฏู ุฏุฑ ูุฑุญููู build ุงูุฌุงู ุดูุฏ (ู ุฏุงุฎู image ูุฑุงุฑ ฺฏุฑุฏ) โ ุจุงุฏ ุฏุณุชูุฑุงุช ุฏุงูููุฏ ุฑุง ุฏุงุฎู Dockerfile ุงุถุงูู ฺฉู. ุงูุง ุงู ฺฉุงุฑ ุจุงุนุซ ุงูุฒุงุด ุฒูุงู build ู ุญุฌู image ุฎูุงูุฏ ุดุฏ.

ุฏุงูููุฏ ูุฏู quickmt-fa-en (ุฏุณุชูุฑุงุช ูพุดููุงุฏ)

ุงฺฏุฑ ูโุฎูุงู ุฏุณุช ุฏุงูููุฏ ฺฉู ุง ุฏุงุฎู build ุงู ฺฉุงุฑ ุงูุฌุงู ุดูุฏุ ุงู ุณู ูุฑูุงู ุฑุง ุงุฌุฑุง ฺฉู (ูุฑุถ ุจุฑ ุงู ุงุณุช ฺฉู ุฏุฑ ูพูุดูู ูพุฑูฺู ูุณุช):

# ุงฺฏุฑ quickmt repo ุฑุง ูุฏุงุฑ:
git clone https://github.com/quickmt/quickmt.git

# ูุตุจ package ูุญู quickmt (ุจุฑุง ุฏุณุชุฑุณ ุจู quickmt-model-download)
pip install ./quickmt/

# ุฏุงูููุฏ ูุฏู quickmt-fa-en ุฏุฑ ูุณุฑ ./quickmt-fa-en
quickmt-model-download quickmt/quickmt-fa-en ./quickmt-fa-en


ุงฺฏุฑ ูโุฎูุงู ุฏุงูููุฏ ุฑุง ุฏุฑ ุฏุงุฎู image ุงูุฌุงู ุฏูุ ูโุชูุงู ููู ุณู ุฏุณุชูุฑ ุฑุง ุฏุฑ Dockerfile ุงุฌุฑุง ฺฉู โ ุงูุง ุญุฌู ููุง image ุฒุงุฏ ูโุดูุฏ.

ููุงุด ูพุดุฑูุช ุฏุงูููุฏ (ุชูุฑุจ)

ุจุนุถ ุงุจุฒุงุฑูุง ุฎุฑูุฌ ฺฉ ููุงุฑ ูพุดุฑูุช ุชููุฏ ููโฺฉููุฏ ุง ุฏุฑ ูุงฺฏ ฺฉุงูุชูุฑ ุจูโุตูุฑุช ุฎูุงูุง ููุงุด ุฏุงุฏู ููโุดููุฏ. ุฏู ุฑุงูููุง:

ูุณุชูู ุฏุฑ ูุฒุจุงู ุฏุงูููุฏ ฺฉู (ุชูุตู): ุฑู ูุงุดู ูุญู ุง ุณุฑูุฑุ ูุฏู ุฑุง ุฏุงูููุฏ ฺฉู ู ูพูุดูู quickmt-fa-en ุฑุง ุฏุฑ volumeู ollama ุง ุฏุฑ ูพุฑูฺู ูุฑุงุฑ ุจุฏู ุชุง ฺฉุงูุชูุฑ ููฺฏุงู ุฑุงูโุงูุฏุงุฒ ูุฌุจูุฑ ุจู ุฏุงูููุฏ ูุดูุฏ.

ุงุณฺฉุฑูพุช progress_downloader.py (ููููู): ฺฉ ุงุณฺฉุฑูพุช ูพุงุชูู ฺฉู ูุงูโูุง ูุฏู ุฑุง ุชฺฉโุจูโุชฺฉ ุงุฒ huggingface ุง ุงุฒ ูุณุช ูุงูโูุง ุฏุงูููุฏ ู ุงูุฏุงุฒูู ูุฑ ูุงู ุฑุง ูุงฺฏ ูโฺฉูุฏ โ ุงู ุฎุฑูุฌ ุฏุฑ ูุงฺฏ ฺฉุงูุชูุฑ ุจูุชุฑ ูุดุงู ุฏุงุฏู ูโุดูุฏ ู ุชูุฑุจุงู ูุดุงู ูโุฏูุฏ ฺูุฏุฑ ุฏุงูููุฏ ุดุฏู (ุชุฎูู).

(ุฏุฑ ุตูุฑุช ุฎูุงุณุชุชุ ูู progress_downloader.py ุขูุงุฏู ู ุจุฑุงุช ูโุณุงุฒู.)

GPU โ ูุนุงูโุณุงุฒ ู ุงุฏุฏุงุดุชโูุง

ุจุฑุง ุงุณุชูุงุฏู ุงุฒ GPU ุจุง Ollama:

ุฑู ูุฒุจุงูุ ุฏุฑุงูุฑ NVIDIA ุฑุง ูุตุจ ฺฉู (ุจุง ูุณุฎูโุง ฺฉู ฺฉุงุฑุชุช ูพุดุชุจุงู ูโฺฉูุฏ).

nvidia-container-toolkit ุฑุง ูุตุจ ฺฉู:

Ubuntu ูุซุงู:

distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker


ฺฉุงูุชูุฑ Ollama ุฑุง ุจุง GPU ุงุฌุฑุง ฺฉู:

ุจุง docker run:

docker run --gpus all -e OLLAMA_USE_GPU=1 --rm -p 11434:11434 ollama/ollama:latest


ุง ุจุง Docker Compose: ุงฺฏุฑ ุณุณุชู Docker ู Compose ุชูุงูุง device_requests ุฑุง ุฏุงุฑุฏุ ุงุฒ deploy.resources ุง device_requests ุงุณุชูุงุฏู ฺฉู. ุฏุฑ ุจุณุงุฑ ุงุฒ ุญุงูุงุช ุณุงุฏูโุชุฑ ุงุณุช ุชุง ุงุฒ docker run --gpus all ุจุฑุง ุชุณุช ุงููู ุงุณุชูุงุฏู ฺฉู.

ุชูุฌู: ุชูุธูุงุช GPU ุฏุฑ docker-compose ููฺฉู ุงุณุช ุจุณุชู ุจู ูุณุฎูู Docker/Compose ูุชูุงูุช ุจุงุดุฏ. ุงฺฏุฑ ูุดฺฉู ุฏุฏุ ุณุฑุนโุชุฑู ุฑุงู ุชุณุชู GPU ุงู ุงุณุช ฺฉู Ollama ุฑุง ุจุง docker run --gpus all ... ุงุฌุฑุง ฺฉู.

ูฺฉุงุช ุฏุฑุจุงุฑูู ูุนูุงุฑ ุชุฑุฌูู ู ุญูุธ ฺฉุฏ

ุงูฺฏูุฑุชู ูพุดููุงุฏ ุจุฑุง ุชุฑุฌููู ุงูู:

ูุชู ุฑุง ุจู ุจุฎุดโูุง fenced code block ู ูุชู ุนุงุฏ ุชูุณู ฺฉู (ูุซูุงู ุจุง regex ุฏูุจุงู ... ุจุงุด).

ุจุฑุง ูุณูุชโูุง ูุชูู ุนุงุฏุ inline code (ูุซู foo()) ุฑุง ุจุง placeholder ุฌุงฺฏุฒู ฺฉู.

ููุท ูุชูู ุทุจุน ุฑุง (ุจุฎุดโูุง ฺฉู ุงุญุชูุงูุงู ูุงุฑุณ ูุณุชูุฏ) ุจู quickmt-fa-en ุจุฏู ู ุชุฑุฌููู ุงูฺฏูุณ ุจฺฏุฑ.

ุฌุงฺฏุฒูโูุง ุฑุง ุจุฑฺฏุฑุฏุงู ู ูุฌููุนูู ููุง ุฑุง ุจู Ollama ุจูุฑุณุช.

ุงฺฏุฑ ูุงุฒ ุจู ุชุฑุฌููู ุฎุฑูุฌ ูุฏู ุจู ูุงุฑุณ (EnโFa) ุฏุงุดุชุ ูโุชูุงู ุงุฒ ููุงู ูุฏู ุง ุงุฒ prompt-engineering ุงุณุชูุงุฏู ฺฉู.

ุงู ุฑูฺฉุฑุฏ ุจุงุนุซ ูโุดูุฏ ฺฉู ฺฉุฏูุง ุจุฏูู ุชุบุฑ ุจุงู ุจูุงููุฏ ู ููุท ูุชู ุทุจุน ุชุฑุฌูู ุดูุฏ โ ุจุฑุง ุณุคุงูโูุง ูู ุญุงุช ุงุณุช.

ุชุบุฑ ูุฏู Ollama

ุจูโุฑุงุญุช ููุฏุงุฑ OLLAMA_MODEL ุฑุง ุนูุถ ฺฉู. ูุซุงู:

# ุฏุฑ main.py ุง environment
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5-coder:1.5b")


ูุณุช ูุฏูโูุง Ollama ุฑุง ุฏุฑ ุณุงุช ุฑุณู Ollama ุจุจู ู ูุฏู ุฏูุฎูุงู ุฑุง ุณุช ฺฉู.

ุชูุตูโูุง ุณุณุชูโุนุงูู / ูุตุจโูุง

ูพุดููุงุฏ ูู: Ubuntu (ุขุฎุฑู LTS) ุง ุณุณุชู ูุจุชู ุจุฑ Debian ุจุฑุง ุณุฑูุฑ.

ุจุฑุง ุชูุณุนู ุฑู ููุฏูุฒ: ุงุฒ WSL2 ุจุง Docker Desktop ู GPU passthrough (ุฏุฑ ุตูุฑุช ูุงุฒ) ุงุณุชูุงุฏู ฺฉู.

ุญุชูุงู nvidia-driver ู nvidia-container-toolkit ุฑุง ุจุฑุง GPU ูุตุจ ฺฉู.

ูุงูโูุง ฺฉูฺฉ ูพุดููุงุฏ ฺฉู ูโุชููู ุขูุงุฏู ฺฉูู (ุจฺฏู ุชุง ุจุณุงุฒู)

progress_downloader.py โ ุฏุงูููุฏ ูุงู ุจูโุตูุฑุช ุชฺฉโุจูโุชฺฉ ุจุง ููุงุด ุงูุฏุงุฒูโูุง/ูพุดุฑูุช ุชูุฑุจ

translate_utils.py โ ุชูุงุจุน tokenization / detokenize / mask-code-blocks ฺฉู ุจุฑุง ุญูุธ ฺฉูุช ุชุฑุฌูู ุงุณุชูุงุฏู ูโฺฉู

main.py ฺฉุงูู ู ููุงููฺฏ ุดุฏู ุจุง entrypoint ู ูุณุฑ ูุฏู

ูุณุฎูู README.html ุชุนุงูู (ุงฺฏุฑ ูโุฎูุงู ุตูุญูู ุฒุจุง ุจุฑุง ููุงุด ุฏุฑ ูุฑูุฑฺฏุฑ ูู ุฏุงุดุชู ุจุงุด)

ุดุจฺฉูโูุง / ุชูุงุณ

Twitter/X: @sepy_dev

GitHub: sepehr.ramzany

Instagram: sepehr.ramzany

Email: sepehr.ramzany@gmail.com

License / ูุชู ุจุงุฒ

ุงู ูพุฑูฺู ุจุฑุง ุงุณุชูุงุฏูู ุดุฎุต ู ุชุญุช ูุฌูุฒ ูุชูโุจุงุฒ ููุชุดุฑ ฺฉู (ูุซูุงู MIT). ุงฺฏุฑ ูโุฎูุงู ูุงู LICENSE ุจุณุงุฒู ูู ุจฺฏู.

๐ฌ๐ง English section (concise summary)
Razor-AI โ Local Ollama Chat with high-quality FaโEn translation

Razor-AI improves Persian usability of Ollama models by translating Persian inputs to English using a local offline translator (quickmt-fa-en, CTranslate2). The process preserves fenced code blocks and inline code, sends the translated English prompt to Ollama, and returns the model reply. Deploy with Docker (Ollama + razor-api). Toggle translation per message in UI.

Quick start
git clone https://github.com/your/repo.git
docker compose build --no-cache
docker compose up -d
open http://localhost:8000

Change Ollama model

Set env var OLLAMA_MODEL (e.g. qwen2.5-coder:1.5b) either in docker-compose.yml or on the runtime.

GPU notes

Install NVIDIA drivers + nvidia-container-toolkit on the host. For quick testing:

docker run --gpus all -e OLLAMA_USE_GPU=1 -p 11434:11434 ollama/ollama:latest


Use GPU-aware Compose/deployment if you need production orchestration. Ollama falls back to CPU if no GPU is available.

Model download (recommended)

Prefer downloading models on host and mounting into container, or let entrypoint.sh download quickmt-fa-en at first runtime. Model size: hundreds of MBs โ initial download may take several minutes.
